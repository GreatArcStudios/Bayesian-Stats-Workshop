{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bayesian Statistics Workshop - Applied Portion\n",
    "### Hosted by: Eric Zhu | [ezhu.build](https://ezhu.build)\n",
    "---\n",
    "The workshop slides will be available on my [Github](https://github.com/GreatArcStudios), which you should be able to find in the same directory as this notebook.\n",
    "\n",
    "## Aims\n",
    "- Creating two Bayesian models\n",
    "- Understanding some pros/cons of Bayesian methods\n",
    "- Some basic prior setting\n",
    "- Evaluating Bayesian model performance\n",
    "- Understanding how to create predictions through the posterior predictive distribution\n",
    "\n",
    "## Breakdown\n",
    "- Creating a multiple imputation model (a model that generates a distribution for the \"population\" and uses that distribution to generate a distribution for our quantity of interest) with random effects.\n",
    "- The data, while simulated, is provided in the context that a large university is trying to analyse student marks in a large first year class (think Convocation (Con) Hall classes at UofT). In this scenario, the University asked the students to voluntarily participate in a pre-study exam to examine the effects of studying. This pre-study exam is equivalent in skill & difficulty wise to the exam everyone gets. Additionally, the University has access to every student's high school GPA, lecture section, and amount of time studied (perhaps they need to use a VPN to connect to the student portal).\n",
    "- The student GPAs are on a 4.0 scale, and generally exam scores are within the range of 0-110 due to bonus marks. Ideally, we would use a truncated normal distribution, but for the sake of time we will not cover it.\n",
    "\n",
    "## Credits\n",
    "\n",
    "While I  created the materials/scenario for this workshop, I must also credit [Professor Dan Simpson](https://dpsimpson.github.io/) for the inspiration. This workshop was based off of a similar scenario from Homework 2 of the Winter 2021 offering of [STA365](https://www.statistics.utoronto.ca/sites/www.statistics.utoronto.ca/files/STA365H1%20sylabus2021%20%283%29.pdf) at the University of Toronto (UofT). We are not explicitly covering post stratification or truncated distributions in the workshop for the sake of time/accessibility. Additionally, explanations of random effect models generally follow those given by [Professor Liza Bolton](https://www.dataembassy.co.nz/) during the Winter 2021 offering of [STA303/1002](https://www.statistics.utoronto.ca/sites/www.statistics.utoronto.ca/files/STA303-1002_syllabus_W21.pdf)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# packages we need in this workshop\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cmdstanpy\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "sns.set_style('whitegrid')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 0: Generating the Data\n",
    "In this workshop, we'll be modelling some generated data. While this means we have a population distribution (the distribution from which we generate from), you'll get to see how Bayesian methods perform in (assumed) frequentist settings, which is pretty cool!\n",
    "\n",
    "For later: we will be leaving half of the data in one column out in the data I provide since we will be creating a model to predict that data.\n",
    "\n",
    "### Important: don't look at this section until the end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# init the numpy array to 0s (dataset will be 4-dim)\n",
    "samples = 5000\n",
    "data = np.zeros((samples, 5))\n",
    "rng = np.random.default_rng(seed=123)\n",
    "# certain lecture sections have on average higher scores\n",
    "section_dict = {\"LEC0101\": 5, \"LEC0202\": 0, \"LEC0303\": -1.25, \"LEC0404\": 2}\n",
    "lecture_sections = rng.choice([\"LEC0101\", \"LEC0202\", \"LEC0303\", \"LEC0404\"], samples)\n",
    "entrance_gpa = rng.normal(3.85, 0.05, samples)\n",
    "prestudy_scores = []\n",
    "study_times = []\n",
    "poststudy_scores = []\n",
    "def generate_data():\n",
    "    for sample in range(samples):\n",
    "        lec_section = lecture_sections[sample]\n",
    "        prestudy_scores.append(rng.normal(57+0.45*entrance_gpa[sample]+section_dict[lec_section], 10, 1))\n",
    "        study_times.append(rng.normal(10+section_dict[lec_section], 2, 1))\n",
    "        poststudy_scores.append(rng.normal(58+0.35*entrance_gpa[sample] + 0.5*prestudy_scores[sample] + study_times[sample]*1.2+section_dict[lec_section], 3.25, 1))\n",
    "\n",
    "generate_data_vectorized = np.vectorize(generate_data)\n",
    "\n",
    "generate_data_vectorized()\n",
    "data[:,1] = prestudy_scores\n",
    "data[:,2] = entrance_gpa\n",
    "data[:,3] = study_times\n",
    "data[:,4] = poststudy_scores\n",
    "data_df = pd.DataFrame(data, columns=[\"Lecture_Section\", \"PreStudy_Scores\",\"Entrance_GPA\", \"Study_Times\", \"PostStudy_Scores\"])\n",
    "data_df = data_df.round(2)\n",
    "data_df.iloc[:,0] = lecture_sections\n",
    "fully_labelled_data = data_df.iloc[:1000]\n",
    "#print(data_df)\n",
    "fully_labelled_data.to_csv(\"full_labelled_workshop_data.csv\")\n",
    "missing_data = data_df.iloc[:1000]\n",
    "missing_data = missing_data[['Lecture_Section', 'Entrance_GPA', 'Study_Times']]\n",
    "poststudy_scores_keep = pd.Series(data_df.PostStudy_Scores[:500])\n",
    "prestudy_scores_keep = pd.Series(data_df.PreStudy_Scores[:500])\n",
    "missing_data.insert(1, \"PreStudy_Scores\", prestudy_scores_keep)\n",
    "missing_data.insert(1, \"PostStudy_Scores\", poststudy_scores_keep)\n",
    "missing_data.to_csv(\"missing_data_workshop_data.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python3\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: Creating a simple model\n",
    "We will use this model as the model that creates a population distribution for the missing data, i.e., we will predict the missing data.\n",
    "\n",
    "To do so, we will use a random effects model, specifically a random effect for $\\texttt{lecture_section}$:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\texttt{PreStudy_Scores}_i &= \\beta_0 + \\beta_1 \\cdot \\texttt{Entrance_GPA}_i + U_{ij} + \\epsilon \\\\\n",
    "\\beta_0 &\\sim N(\\mu_{\\beta_0}, \\tau_{\\beta_0}) \\\\\n",
    "\\beta_1 &\\sim N(\\mu_{\\beta_1}, \\tau_{\\beta_1}) \\\\\n",
    "U &\\sim N(0, \\tau_U) \\\\\n",
    "\\tau_{U} &\\sim N_+(\\mu_{\\tau_U}, \\tau_{\\tau_U}) \\\\\n",
    "\\sigma &\\sim N_+(0, \\tau_{\\sigma})\n",
    "\\end{aligned}\n",
    "\n",
    "Note that $U_{ij}$ is the random effect for the $j^{th}$ lecture section that corresponds to the $i^{th}$ individual. Additionally, with a normally distributed error term recall that for models like linear regression, we may represent the model as the likelihood (with $X$ as the $n$ by $p$ design matrix and $w$ as $p$-dim vector of weights): $ y \\sim N(Xw, \\sigma^2)$.\n",
    "\n",
    "In $\\texttt{lme4}$ style syntax, we may write:\n",
    "\n",
    "$\\texttt{PreStudy_Scores}_i  = \\texttt{Entrance_GPA}_i  + (1~|~\\texttt{Lecture_Section}_i )$\n",
    "\n",
    "So we will need 4 priors: $\\beta_0, \\beta_1, \\tau_U, \\sigma^2$.\n",
    "\n",
    "For the first prior, the prior for the intercept, you know that on average students score around a 55% without studying. Although some students are quite good (or perhaps lucky) and score up to 20% higher without studying\n",
    "\n",
    "As for the prior on $\\beta_1$, we know that a student's High School or Secondary Education GPA has a mild effect on their scores before any studying. Perhaps something like a factor of 0.5, although since we know the effect of GPA isn't that large, we don't expect the effect to be sensibly more than twice the average effect (the centre of your prior distribution).\n",
    "\n",
    "Then for the standard deviation of the random intercept, we know that the intercept can vary in extreme cases by up to 5, i.e., sometimes the lecture section may increase a student's performance by 5 percent! So then, you should expect the lecture section to affect student performance by at least a couple of percent on average, perhaps $\\mu_{\\tau_U} \\approx 2$.\n",
    "\n",
    "Finally, for $\\sigma^2$, the variance of the data, we expect that the distribution is centred around 0. In other words, we can expect our model to be centred around our mean. But the standard deviation of $\\sigma^2$ is then rather large, perhaps we could see students who perform up to 20 or 30 percent better holding all other factors constant!\n",
    "\n",
    "### Remember to use half normal distributions for standard derivation parameters since normal distribution standard deviations are non-negative.\n",
    "\n",
    "## Code!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read the data\n",
    "workshop_data_missing = pd.read_csv(\"missing_data_workshop_data.csv\", index_col=0)\n",
    "#convert to numerical factor levels\n",
    "workshop_data_missing['LecSec_Numeric'] = [code+1 for code in pd.Categorical(workshop_data_missing.Lecture_Section).codes]\n",
    "workshop_data_missing\n",
    "\n",
    "labelled_data = workshop_data_missing[:500]# get the first two hundred rows and all columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Quick EDA, NOT used for determining priors!\n",
    "\n",
    "# Pre-study scores are roughly normally distributed, as expected\n",
    "prestudy_plot = plot_distribution(workshop_data_missing.dropna().PreStudy_Scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Post-study scores\n",
    "poststudy_plot = plot_distribution(workshop_data_missing.PostStudy_Scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of lecture sections\n",
    "pd.value_counts(labelled_data.Lecture_Section)\n",
    "lecsec_plot = sns.histplot(labelled_data.LecSec_Numeric, discrete=True, color=\"#03396c\", edgecolor=\"#011f4b\", alpha=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "prestudy_model = cmdstanpy.CmdStanModel(stan_file=\"workshop_prestudy.stan\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the data lists for Stan\n",
    "data_list_prestudy = {'N':500,\n",
    "                      'prestudy_scores': list(labelled_data.PreStudy_Scores),\n",
    "                      'J_lecsec': len(labelled_data.Lecture_Section.unique()),\n",
    "                      'lec_sec': list(labelled_data.LecSec_Numeric),\n",
    "                      'entrance_gpa': list(labelled_data.Entrance_GPA),\n",
    "                      'mu_sigma' : 0,\n",
    "                      'tau_sigma': 25,\n",
    "                      'mu_beta1': 0.5,\n",
    "                      'tau_beta1': 0.15,\n",
    "                      'mu_intercept': TODO,\n",
    "                      'tau_intercept': TODO,\n",
    "                      'mu_tau_lecsec': 2,\n",
    "                      'tau_tau_lecsec': 1.25,\n",
    "                      'only_prior': 1\n",
    "                      }\n",
    "# fit the model\n",
    "prior_prestudy = prestudy_model.sample(data=data_list_prestudy,\n",
    "                                       parallel_chains=4,\n",
    "                                       iter_sampling=1000,\n",
    "                                       iter_warmup=1000,\n",
    "                                       refresh=500,\n",
    "                                       show_progress=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_list_prestudy['only_prior'] = 0\n",
    "posterior_prestudy = prestudy_model.sample(data=data_list_prestudy,\n",
    "                                       parallel_chains=4,\n",
    "                                       iter_sampling=1000,\n",
    "                                       iter_warmup=1000,\n",
    "                                       refresh=500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prestudy_az = az.from_cmdstanpy(\n",
    "    posterior=posterior_prestudy,\n",
    "    posterior_predictive=\"y_pred\",\n",
    "    observed_data={'y_pred': list(data_list_prestudy['prestudy_scores'])},\n",
    "    log_likelihood=\"log_lik\",\n",
    "    prior = prior_prestudy,\n",
    "    prior_predictive=\"y_pred\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model checking/evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prior draws\n",
    "prior_prestudy_draws = prior_prestudy.draws_pd()\n",
    "prior_prestudy_draws\n",
    "\n",
    "# plot posterior pred. KDE estimates\n",
    "# subset columns we want to plot\n",
    "prestudy_prior_pred_subset = prior_prestudy_draws.filter(like=\"y_pred\").sample(n=9,axis='columns')\n",
    "plot_pred_distributions(prestudy_prior_pred_subset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# posterior draws\n",
    "posterior_prestudy_draws = posterior_prestudy.draws_pd()\n",
    "posterior_prestudy_draws\n",
    "\n",
    "# plotting, we will perform posterior checks and predictive checks\n",
    "\n",
    "# plot the comparison plots\n",
    "# beta_0 comparison plot\n",
    "plot_prior_posterior(prior_prestudy_draws.beta_0, posterior_prestudy_draws.beta_0, parameter_name=\"beta_0\")\n",
    "# beta_1 comparison plot\n",
    "plot_prior_posterior(prior_prestudy_draws.beta_1, posterior_prestudy_draws.beta_1, parameter_name=\"beta_1\")\n",
    "# random effect st dev. comparison plot\n",
    "plot_prior_posterior(prior_prestudy_draws.tau_lecsec, posterior_prestudy_draws.tau_lecsec, parameter_name=\"tau_lecsec\")\n",
    "\n",
    "# plot the posterior pred. distribution ppc\n",
    "az.plot_ppc(prestudy_az)\n",
    "\n",
    "# plot posterior pred. KDE estimates\n",
    "# subset columns we want to plot\n",
    "prestudy_posterior_pred_subset = posterior_prestudy_draws.filter(like=\"y_pred\").sample(n=9,axis='columns')\n",
    "plot_pred_distributions(prestudy_posterior_pred_subset)\n",
    "\n",
    "# plot the PSIS plot\n",
    "az.plot_khat(az.loo(prestudy_az, pointwise=True))\n",
    "\n",
    "# summaries\n",
    "az.summary(prestudy_az)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2: Creating the complex model\n",
    "\n",
    "Now we use the predicted population distribution to construct the distribution of post-study scores. We must use many samples of each lecture section to accurately predict the post-study score. The core idea here is that we must incorporate all of our information about pre-study scores into our distribution for the post-study scores; in other words, we marginalize over the pre-intervention scores.\n",
    "\n",
    "Then, our model will be:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\texttt{PostStudy_Scores}_i &= \\beta_0 + \\beta_1 \\cdot \\texttt{Entrance_GPA}_i + \\\\\n",
    "&\\beta_2 \\cdot \\texttt{PreStudy_Scores}_i + \\beta_3 \\cdot \\texttt{Study_Times}_i + U_{ij} + \\epsilon \\\\\n",
    "\\beta_0 &\\sim N(\\mu_{\\beta_0}, \\tau_{\\beta_0}) \\\\\n",
    "\\beta_1 &\\sim N(\\mu_{\\beta_1}, \\tau_{\\beta_1}) \\\\\n",
    "\\beta_2 &\\sim N(\\mu_{\\beta_2}, \\tau_{\\beta_2}) \\\\\n",
    "\\beta_3 &\\sim N(\\mu_{\\beta_3}, \\tau_{\\beta_3}) \\\\\n",
    "U &\\sim N(0, \\tau_U) \\\\\n",
    "\\tau_{U} &\\sim N_+(\\mu_{\\tau_U}, \\tau_{\\tau_U})\\\\\n",
    "\\sigma &\\sim N_+(0, \\tau_{\\sigma})\n",
    "\\end{aligned}\n",
    "\n",
    "Note that $U_{ij}$ is the random effect for the $j^{th}$ lecture section that corresponds to the $i^{th}$ individual. Again, with a normally distributed error term recall that for models like linear regression, we may represent the model as the likelihood (with $X$ as the $n$ by $p$ design matrix and $w$ as $p$-dim vector of weights): $ y \\sim N(Xw, \\sigma^2)$.\n",
    "\n",
    "In $\\texttt{lme4}$ style syntax, we may write:\n",
    "\n",
    "$\\texttt{PostStudy_Scores}_i  = \\texttt{PreStudy_Scores}_i + \\texttt{Entrance_GPA}_i  + (1~|~\\texttt{Lecture_Section}_i)$\n",
    "\n",
    "So we will need 6 priors: $\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\tau_U, \\sigma^2$.\n",
    "\n",
    "For the first prior, the prior for the intercept, you know that on average students score around a 55% without studying.\n",
    "\n",
    "As for the prior on $\\beta_1$, we know that a student's High School or Secondary Education GPA has an effect on their scores before any studying. Perhaps something like a factor of 0.3, although since we know the effect of GPA isn't that large, we don't expect the effect to be sensibly more than twice the average effect (the centre of your prior distribution) for almost all students.\n",
    "\n",
    "For the prestudy scores, we know that a student's pre-study scores can be used a useful predictor of post-study scores, e.g., someone who scores highly on the pre-study exam is likely to score highly again. So then, we expect a mild effect perhaps like a factor of 0.4, and it should have a similar standard deviation to $\\beta_1$.\n",
    "\n",
    "Additionally, we know that studying ($\\beta_3$) has an enormous effect on post-study scores. Perhaps in previous years, the University measured the effect to be around 1.2, with generally some small variation in the effect size, i.e., $\\pm 0.2$.\n",
    "\n",
    "Then for the variance of the random intercept, we know that the intercept can vary in extreme cases by up to 5, i.e., sometimes the lecture section may increase a student's performance by 5 percent! So then, you should expect the lecture section to affect student performance by at least a couple of percent on average, perhaps $\\mu_{\\tau_U} \\approx 2$.\n",
    "\n",
    "Finally, for $\\sigma^2$, the variance of the data, we expect that the distribution is centred around 0. In other words, we can expect our model to have no error. But the variance of $\\sigma^2$ is then rather large, perhaps we could see students who perform up to 20 percent better despite all other factors constant!\n",
    "\n",
    "\n",
    "## Code!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compile the model\n",
    "poststudy_model = cmdstanpy.CmdStanModel(stan_file=\"workshop_poststudy.stan\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the data lists for Stan\n",
    "data_list_poststudy = {'N':500,\n",
    "                      'prestudy_scores': list(labelled_data.PreStudy_Scores),\n",
    "                      'J_lecsec': len(labelled_data.Lecture_Section.unique()),\n",
    "                      'lec_sec': list(labelled_data.LecSec_Numeric),\n",
    "                      'entrance_gpa': list(labelled_data.Entrance_GPA),\n",
    "                      'poststudy_scores': list(labelled_data.PostStudy_Scores),\n",
    "                      'study_times': list(labelled_data.Study_Times),\n",
    "                      'mu_sigma' : TODO,\n",
    "                      'tau_sigma': TODO,\n",
    "                      'mu_beta1': TODO,\n",
    "                      'tau_beta1': TODO,\n",
    "                      'mu_beta2': 0.5,\n",
    "                      'tau_beta2': 0.15,\n",
    "                      'mu_beta3': TODO,\n",
    "                      'tau_beta3': TODO,\n",
    "                      'mu_intercept': 55,\n",
    "                      'tau_intercept': 10,\n",
    "                      'mu_tau_lecsec': 2,\n",
    "                      'tau_tau_lecsec': 1.25,\n",
    "                      'only_prior': 1\n",
    "                      }\n",
    "# fit the model\n",
    "prior_poststudy = poststudy_model.sample(data=data_list_poststudy,\n",
    "                                       parallel_chains=4,\n",
    "                                       iter_sampling=1000,\n",
    "                                       iter_warmup=1000,\n",
    "                                       refresh=500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_list_poststudy['only_prior'] = 0\n",
    "posterior_poststudy = poststudy_model.sample(data=data_list_poststudy,\n",
    "                                       parallel_chains=4,\n",
    "                                       iter_sampling=1000,\n",
    "                                       iter_warmup=1000,\n",
    "                                       refresh=500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poststudy_az = az.from_cmdstanpy(\n",
    "    posterior=posterior_poststudy,\n",
    "    posterior_predictive='y_pred',\n",
    "    observed_data={'y_pred': list(data_list_poststudy['poststudy_scores'])},\n",
    "    log_likelihood=\"log_lik\",\n",
    "    prior = prior_poststudy,\n",
    "    prior_predictive= 'y_pred'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model checking/evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prior draws\n",
    "prior_poststudy_draws = prior_poststudy.draws_pd()\n",
    "prior_poststudy_draws\n",
    "\n",
    "# plot posterior pred. KDE estimates\n",
    "# subset columns we want to plot\n",
    "poststudy_prior_pred_subset = prior_poststudy_draws.filter(like=\"mu\").sample(n=9,axis='columns')\n",
    "plot_pred_distributions(poststudy_prior_pred_subset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# posterior draws\n",
    "posterior_poststudy_draws = posterior_poststudy.draws_pd()\n",
    "posterior_poststudy_draws\n",
    "\n",
    "# plotting, we will perform posterior checks and predictive checks\n",
    "\n",
    "# plot the comparison plots\n",
    "# beta_0 comparison plot\n",
    "plot_prior_posterior(prior_poststudy_draws.beta_0, posterior_poststudy_draws.beta_0, parameter_name=\"beta_0\")\n",
    "# beta_1 comparison plot\n",
    "plot_prior_posterior(prior_poststudy_draws.beta_1, posterior_poststudy_draws.beta_1, parameter_name=\"beta_1\")\n",
    "# beta_1 comparison plot\n",
    "plot_prior_posterior(prior_poststudy_draws.beta_2, posterior_poststudy_draws.beta_2, parameter_name=\"beta_2\")\n",
    "plot_prior_posterior(prior_poststudy_draws.beta_3, posterior_poststudy_draws.beta_3, parameter_name=\"beta_3\")\n",
    "# random effect st dev. comparison plot\n",
    "plot_prior_posterior(prior_poststudy_draws.tau_lecsec, posterior_poststudy_draws.tau_lecsec, parameter_name=\"tau_lecsec\")\n",
    "\n",
    "# plot the posterior pred. distribution ppc\n",
    "az.plot_ppc(poststudy_az)\n",
    "\n",
    "# plot posterior pred. KDE estimates\n",
    "# subset columns we want to plot\n",
    "poststudy_posterior_pred_subset = posterior_poststudy_draws.filter(like=\"y_pred\").sample(n=9,axis='columns')\n",
    "plot_pred_distributions(poststudy_posterior_pred_subset)\n",
    "\n",
    "# plot the PSIS plot\n",
    "az.plot_khat(az.loo(poststudy_az, pointwise=True))\n",
    "\n",
    "# summaries\n",
    "az.summary(poststudy_az)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate how studying went\n",
    "We will first need to predict the population distribution for pre-study scores and then use those predictions to predict the distribution for post-study scores. By doing so we are able to consider all of the entrance GPAs, study times, lecture sections, and pre-study scores, i.e., we are able to marginalize out everything and obtain the distribution for just post-study scores."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the predictions\n",
    "We create these outside of Stan since it is easier to do so. It would be possible to do so, but it would be less clean."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first subset the draws dataframes and transpose them so that we have each row corresponding to an individual\n",
    "posterior_prestudy_ppd = posterior_prestudy_draws.filter(like='y_pred').T\n",
    "posterior_poststudy_ppd = posterior_poststudy_draws.filter(like='y_pred').T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_pred_distribution(data_df, population_df, impute_var ='prestudy_scores', predictor_list = [], impute_df = None,  n_samples=20):\n",
    "    \"\"\"\n",
    "    We want to predict the population distribution by drawing n_samples from the PPD.\n",
    "    Choose 20 samples by default because of time constraints and works reasonably well\n",
    "    :param data_df: the posterior pred. distribution df\n",
    "    :param population_df: the population df, i.e., the dataframe constructed from given data\n",
    "    :param n_samples: the number of samples to draw from the PPD\n",
    "    :param predictor_list: the list of predictors we use to create predictions\n",
    "    :param impute_var: the variable we will impute\n",
    "    :param impute_df: the datafram containing the data we impute from\n",
    "    :return: a dataframe of the predicted population distribution\n",
    "    \"\"\"\n",
    "    popn_distribution = pd.DataFrame()\n",
    "    for index, row in population_df.iterrows():\n",
    "        lecture_section = row.LecSec_Numeric\n",
    "        matching_sections = data_df.filter(like=f'u_lecsec[{lecture_section}]')\n",
    "        prediction = pd.Series(np.zeros(shape=(n_samples)))\n",
    "        for predictor, var_name in predictor_list:\n",
    "            # get the vector of posterior weights for each coeff\n",
    "            if 'u_lecsec' in predictor:\n",
    "                posterior_weights = matching_sections.sample(n_samples).iloc[:, 0]\n",
    "            elif predictor == impute_var:\n",
    "                ppd_prediction_draws = impute_df.filter(like='y_pred').T\n",
    "                posterior_weights = data_df[predictor].sample(n_samples) * ppd_prediction_draws.sample(n_samples)\n",
    "            else:\n",
    "                posterior_weights = data_df[predictor].sample(n_samples)\n",
    "                if predictor != 'beta_0':\n",
    "                    posterior_weights = posterior_weights * population_df[var_name].iloc[index]\n",
    "            # generate the prediction\n",
    "            prediction = prediction + posterior_weights.reset_index(drop=True)\n",
    "        popn_distribution = popn_distribution.append(prediction, ignore_index=True)\n",
    "    return popn_distribution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the pre-study population distribution\n",
    "prestudy_popn = create_pred_distribution(posterior_prestudy_draws,\n",
    "                                         workshop_data_missing,\n",
    "                                         predictor_list=[('beta_0',''), ('beta_1','Entrance_GPA'), ('u_lecsec','')])\n",
    "\n",
    "plot_distribution(prestudy_popn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the post-study population distribution\n",
    "poststudy_popn = create_pred_distribution(posterior_poststudy_draws,\n",
    "                                         workshop_data_missing,\n",
    "                                         predictor_list=[('beta_0',''),\n",
    "                                                         ('beta_1','Entrance_GPA'),\n",
    "                                                         ('beta_2', 'PreStudy_Scores'),\n",
    "                                                         ('beta_3', 'Study_Times'),\n",
    "                                                         ('u_lecsec','')])\n",
    "\n",
    "plot_distribution(poststudy_popn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the difference\n",
    "study_diff = poststudy_popn - prestudy_popn\n",
    "plot_distribution(study_diff)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now for the comparison to the simulated data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"full_labelled_workshop_data.csv\", index_col=0)\n",
    "plot_distribution(full_data.PreStudy_Scores)\n",
    "plot_distribution(full_data.PostStudy_Scores)\n",
    "plot_distribution(full_data.PostStudy_Scores - full_data.PreStudy_Scores)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Util functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_prior_posterior(prior_data, posterior_data, parameter_name=\"\"):\n",
    "    sns.distplot(prior_data, color=\"#03396c\", hist_kws=dict(edgecolor=\"#011f4b\", alpha=1), kde=False) #prior\n",
    "    sns.distplot(posterior_data, color=\"#d1e1ec\", hist_kws=dict(edgecolor=\"#b3cde0\", alpha=1), kde=False)# posterior\n",
    "    plt.title(f'{parameter_name} prior (dark), posterior (light) comparison')\n",
    "    plt.xlabel('scores')\n",
    "    plt.ylabel('density')\n",
    "    plt.figure()\n",
    "\n",
    "def plot_distribution(data, parameter_name=\"\"):\n",
    "    sns.distplot(data, color=\"#03396c\", hist_kws=dict(edgecolor=\"#011f4b\", alpha=1), kde=False)# posterior\n",
    "    plt.title(f'{parameter_name}')\n",
    "    plt.xlabel('scores')\n",
    "    plt.ylabel('density')\n",
    "    plt.figure()\n",
    "\n",
    "def plot_pred_distributions(pred_distn_df):\n",
    "    grid_size = math.sqrt(len(pred_distn_df.columns))\n",
    "    plt.tight_layout()\n",
    "    for i in range(len(pred_distn_df.columns)):\n",
    "        data = pred_distn_df.iloc[:, i] # column as a series\n",
    "        plt.subplot(grid_size, grid_size, i+1)\n",
    "        sns.distplot(data, color=\"#03396c\", hist_kws=dict(edgecolor=\"#011f4b\", alpha=1), kde=False)# posterior\n",
    "        plt.title(f'{data.name}')\n",
    "        plt.xlabel('scores')\n",
    "        plt.ylabel('density')\n",
    "    plt.figure()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% python\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "base",
   "language": "python",
   "display_name": "Python (base)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}